{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aa32afd-d5d6-433a-bdbe-2d0723237fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# Loading the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec0753f5-ebba-498f-ad29-8178d8b8740b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['far', 'bo', 'sle', 'pÃº', 'ĠÄįerven', 'ovlÃ¡', 'sku', 'Ġohro', 'zili', 'Ġproti', 'idÃº', 'ce', 'ĠautÃ¡', ',', 'Ġa', 'Ġpreto', 'ĠnÃºdz', 'ovo', 'Ġzapark', 'ovala', 'Ġsvoje', 'Ġauto', 'Ġv', 'Ġmo', 'Äįa', 'rine', '.']\n",
      "Token ID: [5627, 347, 547, 4121, 5220, 26657, 656, 18508, 8934, 1115, 23320, 333, 8373, 16, 266, 668, 16807, 719, 32729, 1108, 781, 1212, 264, 393, 435, 19539, 18]\n"
     ]
    }
   ],
   "source": [
    "# The text we want to tokenize\n",
    "text = \"Farboslepú červenovlásku ohrozili protiidúce autá, a preto núdzovo zaparkovala svoje auto v močarine.\".lower()\n",
    "\n",
    "# Tokenization of the lowercase text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Conversion of tokens to their IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token ID:\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a149ea6a-23c1-4f66-80e0-8d9a00d4cf07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,  5627,   347,   547,  4121,  5220, 26657,   656, 18508,  8934,\n",
      "          1115, 23320,   333,  8373,    16,   266,   668, 16807,   719, 32729,\n",
      "          1108,   781,  1212,   264,   393,   435, 19539,    18,     2,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# Pre-prepared encodings for entering the language model\n",
    "encoding = tokenizer(text, max_length=256, padding='max_length', return_tensors=\"pt\")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "602f26bc-22c6-4599-9ea4-d32c3c07e192",
   "metadata": {},
   "source": [
    "Sample segmented text without byte representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b18eefb8-989e-4298-9db4-ec3d4b184f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokenizer/kodovanie.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dictionary = json.load(f)\n",
    "\n",
    "def decode(tokens):\n",
    "    decoded_tokens = []\n",
    "    for token in tokens:\n",
    "        for k, v in dictionary.items():\n",
    "            if k in token:\n",
    "                token = token.replace(k, v)\n",
    "        decoded_tokens.append(token)\n",
    "    return decoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21649f52-7791-42d5-afe9-0842f53dfefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'far', 'bo', 'sle', 'pú', 'Ġčerven', 'ovlá', 'sku', 'Ġohro', 'zili', 'Ġproti', 'idú', 'ce', 'Ġautá', ',', 'Ġa', 'Ġpreto', 'Ġnúdz', 'ovo', 'Ġzapark', 'ovala', 'Ġsvoje', 'Ġauto', 'Ġv', 'Ġmo', 'ča', 'rine', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(decode(tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9df7ce-f228-4e6b-8750-09201b0bfae9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
